{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "b7a6148282c09aebbe467f16e2a50f8a24f9b448"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'7all.csv\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "\n",
    "check_output([\"ls\", \"input/\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b6aef8f16c1bc3c6df979115383429d0c3f9431"
   },
   "source": [
    "# **In this NoteBook we will use python and its libraries to:**\n",
    "\n",
    "   # 1-  Read Data from csv file,  extract words and then clean it\n",
    "  #  2- Calculate the TF terms frequencies vector from the words\n",
    "  #  3- Extract TF from each text to turn it to vector\n",
    "  #  4- Train ml(machine leraning) model on the vectors and then test the model on other text Data.\n",
    "  #  5- use the vector to summarize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "6055b6bd825dabefd9907e30d87d4e05c1faa3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 4900 lines\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #to read csv files\n",
    "import random \n",
    "import numpy as np #handle operations on arrays easily\n",
    "\n",
    "#from the csv file read all the lines and make a table\n",
    "dataset = pd.read_csv(\"input/7all.csv\", encoding='utf-8', delimiter=',', names=[\"cat\", \"text\"])\n",
    "\n",
    "print(\"dataset has\",len(dataset), \"lines\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64f4247fb8b213b936c230dd3e01dfe68591253e"
   },
   "source": [
    "# Samples from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1da79578b0190572d0dfff8bc7647e9adaf07034",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = dataset.sample(3)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "803244b17a04c19d9a361d078f1a716261a4b43c"
   },
   "source": [
    "# Lets take a look at one text sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b129469d94ac9f452bf763b42a62297f8b0c0d7f"
   },
   "outputs": [],
   "source": [
    "print(samples.iloc[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1f9521428771f7d1ee0b3e20e95eb576397408b8"
   },
   "source": [
    "# Calculate how many line does every category has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cec8549c7e2888ba75421b025acc1c80ba56e686"
   },
   "outputs": [],
   "source": [
    "cats = dataset.groupby(\"cat\").size()\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9cc96ab06b1d5e3b040f44560fe66d9a2bab589"
   },
   "source": [
    "# NEXT\n",
    "# Find words frequencies (how many time each word occurs in the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19023f6d2a7ab977def3d2d5dfc7bb86d36b22d4"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_words = []\n",
    "for idx,rows in dataset.iterrows():\n",
    "    text = rows.text\n",
    "    all_words.extend(text.split(\" \"))\n",
    "\n",
    "words_freq = Counter(all_words)\n",
    "\n",
    "print(words_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5df6165e313773ab0d65cd2a8f9f15ebabdb934f"
   },
   "source": [
    "# tools to process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "58fea21a1aafb378ad3644bc23acd7673b1d66ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bir'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#array of stop words to remove stop words from all the texts.\n",
    "import pickle\n",
    "infile = open(\"stopwords\",'rb')\n",
    "stopwords = pickle.load(infile)\n",
    "infile.close()\n",
    "#string of all turkish characters \n",
    "turkishCharaters = \"abcçdefgğhıijklmnoöprsştuüvyz_\"\n",
    "#function use the 'turkishCharaters' list to eliminate symbols and strange characters.\n",
    "def RealWord(string):\n",
    "    if not any(c not in turkishCharaters for c in string):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c4df39f8fcfd549957eea140f05cf86f40933f61"
   },
   "source": [
    "# Lets Start cleaning!!\n",
    "# 1- Extract the word  with frequency bigger than threshold from    'words_freq' Dict \n",
    "# 2- Use the above tools to filter words and characters\n",
    "\n",
    "# 3- [TODO] we could do Stemming or Lemmatizing \n",
    "#  \"gidiyorum\" -> \"gitme - git\" (lemmatizer) or \"gid\" (stemmer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54a421f3c3d98d963450fca45f9dcc0cee6760b9"
   },
   "outputs": [],
   "source": [
    "#we will remove all words that has low frequency \n",
    "threshold = 30\n",
    "words_freq = dict(filter(lambda x: x[1]>threshold, words_freq.items()))\n",
    "#take all words from the dict\n",
    "words = list(words_freq.keys())\n",
    "#remove stop words.\n",
    "filtered_words = [w for w in words if not w in stopwords]\n",
    "#remove non turkish characters.\n",
    "filtered_words = list(filter(RealWord, filtered_words))[1:]\n",
    "print(\"the number of words is\",len(filtered_words),\"word, after we cleaned it.\")\n",
    "print(\"-\"*50)\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c79e2f7b2998098ccc20234b463bff1fb6f4a037"
   },
   "source": [
    "# ****The NEXT CODE IS OPTIONAL****\n",
    "\n",
    "# now we can head back to our previous words_freq dict and see the frequencies of our cleaned words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91e49d12cb8ae28030b2ba3854b2bcb8d467a8d9"
   },
   "outputs": [],
   "source": [
    "#we just need to filter it with our new keys(filterd words)\n",
    "new_freq = dict(filter(lambda x: x[0] in filtered_words, words_freq.items()))\n",
    "#just sort words dict according to its frequencies \n",
    "new_freq = dict(sorted(new_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "print(new_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b4fe7aa1d7a7a6ee1a044e5c1680f412532f1f7"
   },
   "source": [
    "# NOW we cleaned the words and stored it in \"filtered_words\" list\n",
    "\n",
    "# NEXT we will apply TD algorithm on all the text of \"Dataset\":\n",
    "\n",
    "# for every text: < -- LOOP\n",
    "\n",
    " #     1- Create init vector [01,02,...0n] <===> filtered_words [w1,w2...Wn]\n",
    " #     2- Loop through text words. init vector( [0,0....0] ==> [1,3,0,0,5..1])\n",
    " #     3- store init vector in all_vectors list\n",
    "\n",
    "# The idea is to replace every word with a number corresponds to how many times it is occuring in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff0b5e86d012566171540b0fea68733ff9a05195"
   },
   "outputs": [],
   "source": [
    "print(\"for the first ten words:\\n\")\n",
    "print(\"filtered_words:\",filtered_words[:10])\n",
    "print(\"-\"*50)\n",
    "init_vector = [0]*len(filtered_words)\n",
    "print(\"init_vector:\",init_vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b909c66b792a82b06a379c27072357ea168c985"
   },
   "outputs": [],
   "source": [
    "#[0,0,0....0] ==> [1,4,2....2]\n",
    "def create_vector(text):\n",
    "    words_of_text = text.split(\" \")\n",
    "    text_vector = [0]*len(init_vector)\n",
    "    for word in words_of_text:\n",
    "        if word in filtered_words:\n",
    "            idx = filtered_words.index(word)\n",
    "            text_vector[idx] += 1\n",
    "    return text_vector\n",
    "\n",
    "all_vectors = []\n",
    "\n",
    "#loop through all texts\n",
    "for idx,row in dataset.iterrows():\n",
    "    text = dataset.loc[idx, \"text\"]\n",
    "    all_vectors.append(create_vector(text))\n",
    "\n",
    "\n",
    "print(\"we now have\",len(all_vectors),\"vector.\")\n",
    "print(\"these vectors will be our input data to our model.\")\n",
    "print(\"-\"*50)\n",
    "print(\"first vector:\",all_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73dd15a4c1d200333ceffea991231859baeaea72"
   },
   "source": [
    "# NOW we created vectors for all the texts and we grouped it in \"all_vectors\"\n",
    "# NEXT train *SUPERVISED* machine learning model on \"all_vectors\":\n",
    "\n",
    "#  supervised means maping inputs to outputs (X => Y) we have inputs which is \"all_vectors\" and if you looked at the \"dataset\"(the csv file) we see that each line has (\"text\",\"cat\") so you can take the category of each line as its label.\n",
    "# so the mapping function will be like this:\n",
    "# for every ( *vector* in all_vectors && (\"text\",\"*cat*\") in \"Dataset\"):\n",
    "\n",
    "#         1.  model_dataset.add(vector,cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3019dc55bfcf7064e37e6c7b09ebd6fa863ae50d"
   },
   "outputs": [],
   "source": [
    "labels = dataset[\"cat\"].tolist()\n",
    "\n",
    "model_dataset = list(zip(all_vectors,labels))\n",
    "\n",
    "print(\"one data sample:\",model_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8dcb0211d265c9c0c4393104fc1ff8f9f986cbf5"
   },
   "source": [
    "# Now we will shuffle \"model_dataset\" and split it to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a17020f894fc360215b6564e3d1d30f5d6708c3"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_dataset_shuf = shuffle(model_dataset)\n",
    "all_vectors_shuf,labels_shuf = zip(*model_dataset_shuf)\n",
    "\n",
    "data_length = len(all_vectors)\n",
    "train_x,train_y = all_vectors_shuf[:int(data_length*2/3)],labels_shuf[:int(data_length*2/3)]\n",
    "test_x,test_y = all_vectors_shuf[int(data_length*2/3):],labels_shuf[int(data_length*2/3):]\n",
    "\n",
    "\n",
    "print(\"total data size:\",data_length)\n",
    "print(\"Train data size:\",len(train_x))\n",
    "print(\"Test data size:\",len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2452b91f6eb455a7b53e991e2ab46e16715f906"
   },
   "source": [
    "# we are ready to train!!\n",
    "# we just need to pick a model, I choose random forest but you can pick any model you want. \n",
    "# So let's train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d88f79c66dbf8e17ad22557a3483872422c192c"
   },
   "outputs": [],
   "source": [
    "print (\"Training the random forest...\")\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_x, train_y )\n",
    "\n",
    "print(\"training is finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9563a6d4edfadb77811da3fa9d06c1be038a9844"
   },
   "source": [
    "# training is finished, now lets take a look at its accuracy and see if the model learned something from the data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e5d70ea3dfaf818efed801736dd3186abe0d79d"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def cal_acc(set_x,set_y):\n",
    "    results = forest.predict(set_x)\n",
    "    cnf_matrix = metrics.confusion_matrix(set_y, results)\n",
    "    sim = 0\n",
    "    for i in range(len(results)):\n",
    "        if set_y[i] == results[i]:\n",
    "            sim += 1\n",
    "    return round(sim/len(results),2)\n",
    "\n",
    "train_accuracy = cal_acc(train_x,train_y)\n",
    "test_accuracy = cal_acc(test_x,test_y)\n",
    "\n",
    "print(\"train accurcy:\", train_accuracy)\n",
    "print(\"test accurcy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1cab7baa5645428ad57cffbdc59e45ceacc359e9"
   },
   "source": [
    "# Great!!, we archived good accuracy on both train and test sets\n",
    "# Now this is a text from the internet it is obivuios that it is \"siyaset\" politcs lets run our model on this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16e6499dcfee9745f3f5970ff2510318550a9d03"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"ak_parti Genel  Başkanı ve Cumhurbaşkanı Recep Tayyip Erdoğan, Hamas Siyasi Büro Başkanı İsmail Heniyye ile telefon görüşmesi gerçekleştirdi.\n",
    "Erdoğan, Hamas lideri Heniyye iler yaptığı telefon görüşmesinde Heniyye'ye, ağabeyi Halid Heniyye’nin vefatı dolayısıyla başsağlığı diledi\"\"\"\n",
    "text_vector = create_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa6135f0858e189704b025b04d2e57af6b950618"
   },
   "outputs": [],
   "source": [
    "results = forest.predict([text_vector])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "073d16415941796dd4e3eedcbd74121c419341b1"
   },
   "source": [
    "# The model predicted right!!\n",
    "# **congratulation** you have now a model that predicts the category of the texts.\n",
    "\n",
    "# we are almost done, now you are just going to implement other simple application using \"filtered_words\" list.\n",
    "\n",
    "# we will do abstraction-based text summrization. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9fa8fba6d71a267071a0b11480bb73e5f9b08531"
   },
   "outputs": [],
   "source": [
    "text_to_summur = \"\"\"Bu durum şu şekilde ortaya çıktı: Samsung Galaxy A8 Star’ın Hong Kong sürümünü tanıtmak amacıyla internet sitesinde yer verilen fotoğrafı çeken fotoğrafçı Dunja Djudjic, fotoğrafının EyeEm aracılığıyla Getty üzerinde satıldığına dair bilgilendirildi ve bunun üzerine hemen tersine bir fotoğraf araması yaptığında bu durumu farketti.\n",
    "Üstelik bu olay, Samsung’un bu tarz bir şeyi ilk kez yapışı değildi. GSM Arena’nın belirttiğine göre, bu yılın başında Samsung Brezilya, Samsung Galaxy A8’i tanıtmak için resmi Twitter hesabından attığı -ve daha sonradan silinen- iki tweet’te stok fotoğraflar kullanmıştı. Ancak görünen o ki, gerçekten söz konusu telefonla çekilmemiş bir fotoğrafın üzerine Samsung filigranı eklemek ve daha sonrasında da bir akıllı telefonun tanıtımını yapmak için bu fotoğrafı kullanmak, Samsung açısından yanıltıcı reklam yapmak anlamına gelmiyor.\n",
    "Samsung’dan Apple ile Dalga Geçen Reklam Filmleri Serisi Buradaki problem, Samsung’un reklamda kullandığı fotoğrafın tanıtımını yaptığı telefon ile çekilmediğini açıkça belirtmiyor olmasıdır. Tabii, bunu belirtmesinin ne kadar mantıklı olacağı da tartışılır. Zira böyle bir durumda birçok kişi, akıllı telefonun kamerasına vurgu yaptığı bir reklamda stok fotoğraf kullandığı için Samsung’u eleştirecektir.\n",
    "Samsung’un yanı sıra, Çinli akıllı telefon üreticisi Huawei de geçtimiz aylarda Huawei Nova i3 model akıllı telefonunu tanıtmak için aynı yöntemi uygulamış ve yakalanmıştı. Huawei ise buna karşılık olarak yaptığı açıklamada, “ürün çekimlerinin sadece referans amaçlı olduğunu” tekrarlayarak bu uygulamayı haklı göstermeye çalışmıştı. Bu açıdan bakacak olursak, bu zamana kadar diğer akıllı telefon üreticilerinin de telefon kamerasına vurgu yaptıkları reklamlarda stok fotoğraf kullanmış olma ihtimalleri de bir hayli yüksek.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39302d608bbf275992ef0f1a81f6481034047df6"
   },
   "source": [
    "# First we need to tokenize text to sentences we will do it using nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f94f3b2244027ff7d77b0d61f44616a49d34d93e"
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "sents = sent_tokenize(text_to_summur)\n",
    "print(\"we have\", len(sents),\"in the text\")\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c5b548aa90c65454f58838234fbdfce09aa5f265"
   },
   "source": [
    "# next we will make TF vector for each sentnce (calculating frequencies of the words of the sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f6f381b6dca770eb9768c9b8113e4dfb1f2ba11"
   },
   "outputs": [],
   "source": [
    "sents_vecs = []\n",
    "for sent in sents:\n",
    "    sents_vecs.append(create_vector(sent))\n",
    "print(sents_vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd84e7917cf0da68a6c5d80152b87efff1f4b64e"
   },
   "source": [
    "# Now lets sum the frequencies of all trems for each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f7fdf1aeab659165581490e7dc90ec0144a513c"
   },
   "outputs": [],
   "source": [
    "sums = []\n",
    "for vec in sents_vecs:\n",
    "    sums.append(sum(vec))\n",
    "\n",
    "list_to_print = [\"sent{}\".format(i) for i in range(len(sents))]\n",
    "print(list_to_print)\n",
    "print(sums)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f26bcb6800350126a292440a51d4fb2cc3f70ad"
   },
   "source": [
    "# after we sumed up all terms frequencies for each vector we choose the indices which are not zeros in our case [3,6,8]\n",
    "# and finaly we choose the sents (3,6,8) as the summarization of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c20bc01339172fe2baf69529101f6e1b5783d1c"
   },
   "outputs": [],
   "source": [
    "idx = np.nonzero(sums)[0]\n",
    "sents = np.array(sents)\n",
    "\n",
    "print(\"summarization:\")\n",
    "print(\"-\"*50)\n",
    "print(\"\\n\".join(sents[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c14615b7cd91ac287688325f141afc8f0b56b63e"
   },
   "source": [
    "# **WE ARE DONE !!** that's all.\n",
    "# I want thank you for completing this tutorial and i hope you found it usefull and well organized, and I wish you a great day.\n",
    "\n",
    "# Happy coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "72bb9d67750d18fb3b4d71e17bcea303c48aa2e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
